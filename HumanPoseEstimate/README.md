# Human Pose Estimation

The objective of this assignment is to implement Simple Baseline Pose Estimation to estimate Human pose in an image and deploy the model on AWS.

The model implementation is based on the paper [Simple Baseline for HPE and tracking](https://github.com/Microsoft/human-pose-estimation.pytorch). Pre-trained model (trained on POSE MPII dataset) has been used.

## Simple Baseline for Human Pose Estimation

### Model Architecture

The Pose Estimation model uses ResNet for image feature extraction and few deconvolutional layers to estimate the pose heatmaps.

To generate heatmaps, three deconvolutional layers with batch normalization and ReLU activation are used. Each layer has 256 filters with 4 × 4 kernel. The stride is 2. A 1 × 1 convolutional layer is added at last to generate predicted heatmaps `{H1 . . . Hk}` for all k key points. Each of the final 16 channel feature maps predicts a keypoint. The paper refers to this method as the simplest way to estimate heat maps from deep and low resolution feature maps.

![](https://github.com/akshatjaipuria/AWS-Deployment/blob/master/HumanPoseEstimate/images/model_architecture.JPG)

### Loss Function

Mean Squared Error (MSE) is used as the loss between the predicted heatmaps and targeted heatmaps. The targeted heatmap for joint k is generated by applying a 2D gaussian centered on the kth joint’s ground truth location.


```
class JointsMSELoss(nn.Module):
    def __init__(self, use_target_weight):
        super(JointsMSELoss, self).__init__()
        self.criterion = nn.MSELoss(size_average=True)
        self.use_target_weight = use_target_weight

    def forward(self, output, target, target_weight):
        batch_size = output.size(0)
        num_joints = output.size(1)
        heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)
        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)
        loss = 0

        for idx in range(num_joints):
            heatmap_pred = heatmaps_pred[idx].squeeze()
            heatmap_gt = heatmaps_gt[idx].squeeze()
            if self.use_target_weight:
                loss += 0.5 * self.criterion(
                    heatmap_pred.mul(target_weight[:, idx]),
                    heatmap_gt.mul(target_weight[:, idx])
                )
            else:
                loss += 0.5 * self.criterion(heatmap_pred, heatmap_gt)

        return loss / num_joints
```


## Results

Shared below is the Human Pose estimate from the model.

<TABLE>
  <TR>
    <TH>Input Image</TH>
    <TH>Human Pose Estimate</TH>
  </TR>
   <TR>
      <TD><img src="https://github.com/akshatjaipuria/AWS-Deployment/blob/master/HumanPoseEstimate/images/charlie_sheen.jpg" alt="input_image"
	title="inp_img" width="300" height="400" /></TD>
      <TD><img src="https://github.com/akshatjaipuria/AWS-Deployment/blob/master/HumanPoseEstimate/images/charlie_with_pose.jpg" alt="input_image"
	title="pose_img" width="300" height="400" /></TD>
   </TR>
</TABLE>

