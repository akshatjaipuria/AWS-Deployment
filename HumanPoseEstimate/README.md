# Human Pose Estimation

The objective of this assignment is to implement Simple Baseline Pose Estimation to estimate Human pose in an image and deploy the model on AWS.

The model implementation is based on the paper [Simple Baseline for HPE and tracking](https://github.com/Microsoft/human-pose-estimation.pytorch). Pre-trained model (trained on POSE MPII dataset) has been used.

The paper also addresses the problems encountered while tracking people using object recognition approach. Here, they have proposed and levereged the approach based on human pose and optical tracking. This one is a very cool concept used where we can use bipertite matching to map the same humans from one video frame to another with the help of predicted pose and optical tracking. The same mapped humans are assigned the same id as in the previous frame of the video, and others are assigned a new id. This approach also eliminates, upto an extent, the problem where the human body becomes a little blurry due to movement in subsequent frames and are not identified by a human identifier model.

## Simple Baseline for Human Pose Estimation

### Model Architecture

The Pose Estimation model uses ResNet for image feature extraction and few deconvolutional layers to estimate the pose heatmaps.

To generate heatmaps, three deconvolutional layers with batch normalization and ReLU activation are used. Each layer has 256 filters with 4 × 4 kernel. The stride is 2. A 1 × 1 convolutional layer is added at last to generate predicted heatmaps `{H1 . . . Hk}` for all k key points. Each of the final 16 channel feature maps predicts a keypoint. The paper refers to this method as the simplest way to estimate heat maps from deep and low resolution feature maps.

![](https://github.com/akshatjaipuria/AWS-Deployment/blob/master/HumanPoseEstimate/images/model_architecture.JPG)

### Loss Function: JointsMSELoss
The loss function used here is called JointsMSE Loss. It's a simple loss, calculated by taking the mean of MSELoss between the predicted heatmaps and targeted heatmaps for each key point of the pose. The targeted heatmap for any joint k is generated by applying a 2D gaussian centered on the kth joint's ground truth location. We can also give weightage to the different joints with the help of target_weight parameter, which holds the weights for each joint. IF no weightage is required, the loss would be calculated without any specific weightage.

```python
class JointsMSELoss(nn.Module):
    def __init__(self, use_target_weight):
        super(JointsMSELoss, self).__init__()
        self.criterion = nn.MSELoss(size_average=True)
        self.use_target_weight = use_target_weight

    def forward(self, output, target, target_weight):
        batch_size = output.size(0)
        num_joints = output.size(1)
        heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)
        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)
        loss = 0

        for idx in range(num_joints):
            heatmap_pred = heatmaps_pred[idx].squeeze()
            heatmap_gt = heatmaps_gt[idx].squeeze()
            if self.use_target_weight:
                loss += 0.5 * self.criterion(
                    heatmap_pred.mul(target_weight[:, idx]),
                    heatmap_gt.mul(target_weight[:, idx])
                )
            else:
                loss += 0.5 * self.criterion(heatmap_pred, heatmap_gt)

        return loss / num_joints
```


## Results

Shared below is the Human Pose estimate from the model.

<TABLE>
  <TR>
    <TH>Input Image</TH>
    <TH>Human Pose Estimate</TH>
  </TR>
   <TR>
      <TD><img src="https://github.com/akshatjaipuria/AWS-Deployment/blob/master/HumanPoseEstimate/images/charlie_sheen.jpg" alt="input_image"
	title="inp_img" width="300" height="400" /></TD>
      <TD><img src="https://github.com/akshatjaipuria/AWS-Deployment/blob/master/HumanPoseEstimate/images/charlie_with_pose.jpg" alt="input_image"
	title="pose_img" width="300" height="400" /></TD>
   </TR>
</TABLE>

## Some Fun with Gestures

We have used this Human Pose Estimation mapping to track the gesture of the person and invoke certain functionalities according to the movement of the body parts. For now, your implementation supports playing and stopping music. To play the song, the person needs to raise his/her left hand once, and the music starts. Similarly, to stop the misic the person has to raise the right hand up. The demo video can be found on the following link.
[Demo](https://drive.google.com/file/d/1h3Ka9SY4qQrd-6-rsW3T6xyQgE1gVUdE/view?usp=sharing)

